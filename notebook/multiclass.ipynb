{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import sys\n",
    "sys.path.append('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[200 191 185 181 169 175 144]\n",
      "0.834\t0.791\t0.825\t0.662\t0.843\t0.535\n",
      "0.9\t0.825\t0.837\t0.759\t0.854\t0.636\n",
      "0.91\t0.853\t0.856\t0.858\t0.852\t0.706\n",
      "0.903\t0.863\t0.86\t0.857\t0.864\t0.725\n",
      "0.905\t0.858\t0.827\t0.855\t0.844\t0.714\n",
      "0.923\t0.891\t0.875\t0.91\t0.87\t0.776\n",
      "0.825\t0.763\t0.603\t0.831\t0.603\t0.434\n",
      "Average\n",
      "0.886\t0.835\t0.812\t0.819\t0.818\t0.647\n",
      "------Macro------\n",
      "Macro accuracy 0.834800270819228\n",
      "Macro precision 0.8118102368287554\n",
      "Macro recall 0.8254100652327967\n",
      "Macro f1-score 0.8184453700464129\n",
      "------Micro------\n",
      "Micro accuracy 0.8348002708192281\n",
      "Micro precision 0.8237704918032787\n",
      "Micro recall 0.8398328690807799\n",
      "Micro f1-score 0.8317241379310345\n",
      "------Hamming loss------\n",
      "Hamming loss 0.16519972918077183\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.834800270819228"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from metric import compute_metric_labelwise, compute_metric_score\n",
    "import numpy as np\n",
    "import pickle\n",
    "runtime_list = sorted(os.listdir('../result/binary'))[-1:]\n",
    "folder = '../result/multiclass/2023-09-25_10-34-34'\n",
    "labels, outputs = [], []\n",
    "for k in range(1, 11):\n",
    "    result_file = f'{folder}/model3/{k}.pkl'\n",
    "    res = pickle.load(open(result_file, 'rb'))\n",
    "    output, label = res\n",
    "    labels.append(label)\n",
    "    outputs.append(output)\n",
    "#    compute_metric_labelwise(np.array(label), np.array(output), show_detail = False)\n",
    "labels = np.vstack(labels)\n",
    "outputs = np.vstack(outputs)\n",
    "\n",
    "#outputs\n",
    "print(sum(outputs > 0.01))\n",
    "compute_metric_labelwise(labels, outputs)\n",
    "compute_metric_score(labels, outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((211, 7), (211, 7))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels.shape, outputs.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ACP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
