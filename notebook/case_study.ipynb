{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import sys\n",
    "sys.path.append('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8899\t1.0\tnan\t0.0\tnan\n",
      "0.8991\t1.0\tnan\t0.0\tnan\n",
      "0.8991\t1.0\tnan\t0.0\tnan\n",
      "0.9266\t1.0\tnan\t0.0\tnan\n",
      "0.945\t1.0\tnan\t0.0\tnan\n",
      "Mean\n",
      "0.9266\t1.0\tnan\t0.0\tnan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/s853wang/ACP/DUO-ACP/notebook/../metric.py:26: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  specificity = tn / (tn + fp)\n",
      "/data/s853wang/miniconda3/envs/ACP/lib/python3.9/site-packages/sklearn/metrics/_ranking.py:1123: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
      "  warnings.warn(\n",
      "/data/s853wang/ACP/DUO-ACP/notebook/../metric.py:26: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  specificity = tn / (tn + fp)\n",
      "/data/s853wang/miniconda3/envs/ACP/lib/python3.9/site-packages/sklearn/metrics/_ranking.py:1123: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
      "  warnings.warn(\n",
      "/data/s853wang/ACP/DUO-ACP/notebook/../metric.py:26: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  specificity = tn / (tn + fp)\n",
      "/data/s853wang/miniconda3/envs/ACP/lib/python3.9/site-packages/sklearn/metrics/_ranking.py:1123: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
      "  warnings.warn(\n",
      "/data/s853wang/ACP/DUO-ACP/notebook/../metric.py:26: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  specificity = tn / (tn + fp)\n",
      "/data/s853wang/miniconda3/envs/ACP/lib/python3.9/site-packages/sklearn/metrics/_ranking.py:1123: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
      "  warnings.warn(\n",
      "/data/s853wang/ACP/DUO-ACP/notebook/../metric.py:26: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  specificity = tn / (tn + fp)\n",
      "/data/s853wang/miniconda3/envs/ACP/lib/python3.9/site-packages/sklearn/metrics/_ranking.py:1123: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
      "  warnings.warn(\n",
      "/data/s853wang/ACP/DUO-ACP/notebook/../metric.py:26: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  specificity = tn / (tn + fp)\n",
      "/data/s853wang/miniconda3/envs/ACP/lib/python3.9/site-packages/sklearn/metrics/_ranking.py:1123: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from metric import compute_metric\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "runtime = '2023-09-26_23-20-59'\n",
    "thres = 0.5\n",
    "plt.figure()\n",
    "idx = 4\n",
    "data_dir = f'../datasets/Case-study/binary2.tsv'\n",
    "name = f'model{idx}'\n",
    "pkl_path = f'../result/binary/{runtime}/{name}'\n",
    "results = sorted(os.listdir(pkl_path))\n",
    "num_split = len(results)\n",
    "dataset = pd.read_csv(data_dir, sep='\\t')\n",
    "dataset['seqlen'] = dataset['text'].apply(lambda x: len(x))\n",
    "for i in range(num_split):\n",
    "    pkl_dir = f'{pkl_path}/{results[i]}'\n",
    "    with open(pkl_dir, 'rb') as fr:\n",
    "        pkl = pickle.load(fr)\n",
    "        outputs, labels = pkl\n",
    "        compute_metric(labels[:, 1], outputs[:, 1], thres=thres)\n",
    "    dataset[i] = outputs[:, 1]\n",
    "column_names = range(num_split)\n",
    "averages = dataset[column_names].mean(axis=1)\n",
    "print('Mean')\n",
    "auroc_val, fpr, tpr = compute_metric(labels[:, 1], averages, thres=thres)\n",
    "dataset['avg'] = averages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(109, 7)\n",
      "0.933\t0.835\t0.803\t0.667\t0.871\t0.664\n",
      "0.902\t0.771\t0.887\t0.86\t0.79\t0.56\n",
      "0.96\t0.872\t0.772\t0.797\t0.863\t0.764\n",
      "0.96\t0.89\t0.846\t0.914\t0.846\t0.76\n",
      "0.957\t0.881\t0.905\t0.935\t0.854\t0.757\n",
      "0.981\t0.908\t0.8\t0.91\t0.848\t0.786\n",
      "0.927\t0.789\t0.207\t0.777\t0.343\t0.401\n",
      "Average\n",
      "0.946\t0.849\t0.746\t0.837\t0.774\t0.67\n",
      "------Macro------\n",
      "Macro accuracy 0.8492791612057667\n",
      "Macro precision 0.745595165568407\n",
      "Macro recall 0.8858448972574744\n",
      "Macro f1-score 0.7736454368028346\n",
      "------Micro------\n",
      "Micro accuracy 0.8492791612057667\n",
      "Micro precision 0.7764350453172205\n",
      "Micro recall 0.8624161073825504\n",
      "Micro f1-score 0.8171701112877583\n",
      "------Hamming loss------\n",
      "Hamming loss 0.15072083879423329\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8492791612057667"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from metric import compute_metric_labelwise, compute_metric_score\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "folder = f'../result/multiclass/{runtime}'\n",
    "labels, outputs = [], []\n",
    "for k in range(1, 11):\n",
    "    result_file = f'{folder}/model3/{k}.pkl'\n",
    "    res = pickle.load(open(result_file, 'rb'))\n",
    "    output, label = res\n",
    "    labels.append(label)\n",
    "    outputs.append(output)\n",
    "\n",
    "labels = np.mean(labels, axis=0)\n",
    "outputs = np.mean(outputs, axis=0)\n",
    "print(outputs.shape)\n",
    "\n",
    "#outputs\n",
    "compute_metric_labelwise(labels, outputs)\n",
    "compute_metric_score(labels, outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyteomics import fasta\n",
    "from util import all_labels\n",
    "fa = fasta.read('../datasets/Case-study/multiclass2.fasta')\n",
    "seq_list = []\n",
    "for x in fa:\n",
    "    seq_list.append(x.sequence)\n",
    "    \n",
    "df = pd.DataFrame(outputs, columns=all_labels)\n",
    "df['seq'] = seq_list\n",
    "df['binary'] = averages\n",
    "df = df.sort_values('binary', ascending=False)\n",
    "df.to_csv('out.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ACP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
